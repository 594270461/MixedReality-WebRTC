{
  "api/Microsoft.MixedReality.WebRTC.TaskExtensions.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.TaskExtensions.html",
    "title": "Class TaskExtensions | MixedReality-WebRTC Documentation",
    "keywords": "Class TaskExtensions Collection of extension methods for Task . Inheritance Object TaskExtensions Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public static class TaskExtensions : object Methods | Improve this Doc View Source AsTask(CancellationToken) A simple helper to enable \"awaiting\" a by creating a task wrapping it. Declaration public static Task AsTask(this CancellationToken cancellationToken) Parameters Type Name Description CancellationToken cancellationToken The to await. Returns Type Description Task The task that can be awaited. | Improve this Doc View Source IgnoreCancellation(Task) Prevents or from trickling up. Declaration public static Task IgnoreCancellation(this Task task) Parameters Type Name Description Task task The task to ignore exceptions for. Returns Type Description Task A wrapping task for the given task. | Improve this Doc View Source IgnoreCancellation<T>(Task<T>, T) Prevents or from trickling up. Declaration public static Task<T> IgnoreCancellation<T>(this Task<T> task, T defaultCancellationReturn = null) Parameters Type Name Description Task <T> task The task to ignore exceptions for. T defaultCancellationReturn The default value to return in case the task is cancelled. Returns Type Description Task <T> A wrapping task for the given task. Type Parameters Name Description T The result type of the Task. | Improve this Doc View Source Unless(Task, CancellationToken) The task will be awaited until the cancellation token is triggered. (await task unless cancelled). Declaration public static Task Unless(this Task task, CancellationToken cancellationToken) Parameters Type Name Description Task task The task to await. CancellationToken cancellationToken The cancellation token to stop awaiting. Returns Type Description Task The task that can be awaited unless the cancellation token is triggered. Remarks This is different from cancelling the task. The use case is to enable a calling method bow out of the await that it can't cancel, but doesn't require completion/cancellation in order to cancel it's own execution."
  },
  "api/Microsoft.MixedReality.WebRTC.SignalerMessage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SignalerMessage.html",
    "title": "Class SignalerMessage | MixedReality-WebRTC Documentation",
    "keywords": "Class SignalerMessage Data that makes up a signaler message Inheritance Object SignalerMessage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class SignalerMessage : object Remarks Note: the same data is used for transmitting and receiving Fields | Improve this Doc View Source Data The primary message contents Declaration public string Data Field Value Type Description String | Improve this Doc View Source IceDataSeparator The data separator needed for proper ICE serialization Declaration public string IceDataSeparator Field Value Type Description String | Improve this Doc View Source MessageType The message type Declaration public SignalerMessage.WireMessageType MessageType Field Value Type Description SignalerMessage.WireMessageType | Improve this Doc View Source TargetId The target id to which we send messages Declaration public string TargetId Field Value Type Description String Remarks This is expected to be set when SendMessageAsync(SignalerMessage) is called"
  },
  "api/Microsoft.MixedReality.WebRTC.SignalerMessage.WireMessageType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SignalerMessage.WireMessageType.html",
    "title": "Enum SignalerMessage.WireMessageType | MixedReality-WebRTC Documentation",
    "keywords": "Enum SignalerMessage.WireMessageType Possible message types as-serialized on the wire Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum WireMessageType : int Fields Name Description Answer A SDP answer message Ice A trickle-ice or ice message Offer A SDP offer message Unknown An unrecognized message"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.html",
    "title": "Class PeerConnection | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection The WebRTC peer connection object is the entry point to using WebRTC. Inheritance Object PeerConnection Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class PeerConnection : IDisposable Constructors | Improve this Doc View Source PeerConnection(ISignaler) Construct an uninitialized peer connection object which will delegate to the given ISignaler implementation for its WebRTC signaling needs. Declaration public PeerConnection(ISignaler signaler) Parameters Type Name Description ISignaler signaler The signaling implementation to use. Fields | Improve this Doc View Source Credentials Optional TURN server credentials. Declaration public string Credentials Field Value Type Description String | Improve this Doc View Source PreferredAudioCodec Name of the preferred audio codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredAudioCodec Field Value Type Description String | Improve this Doc View Source PreferredVideoCodec Name of the preferred video codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredVideoCodec Field Value Type Description String | Improve this Doc View Source RemotePeerId Identifier of the remote peer for signaling. This must be sent before the connection starts to send messages via the Signaler . Declaration public string RemotePeerId Field Value Type Description String | Improve this Doc View Source Servers List of TURN and/or STUN servers to use for NAT bypass, in order of preference. Declaration public List<string> Servers Field Value Type Description List < String > | Improve this Doc View Source UserName Optional TURN server username. Declaration public string UserName Field Value Type Description String Properties | Improve this Doc View Source Initialized Boolean property indicating whether the peer connection has been initialized. Declaration public bool Initialized { get; } Property Value Type Description Boolean | Improve this Doc View Source IsConnected Declaration public bool IsConnected { get; } Property Value Type Description Boolean | Improve this Doc View Source Signaler Signaler implementation used by this peer connection, as specified in the constructor. Declaration public ISignaler Signaler { get; } Property Value Type Description ISignaler Methods | Improve this Doc View Source AddDataChannelAsync(String, Boolean, Boolean) Add a new in-band data channel whose ID will be determined by the implementation. A data channel is negotiated in-band when one peer requests its creation to the WebRTC core, and the implementation negotiates with the remote peer an appropriate ID by sending some SDP offer message. In that case once accepted the other peer will automatically create the appropriate data channel on its side with that negotiated ID, and the ID will be returned on both sides to the user for information. Compares to out-of-band messages, this requires exchanging some SDP messages, but avoids having to determine a common unused ID and having to explicitly open the data channel on both sides. Declaration public Task<DataChannel> AddDataChannelAsync(string label, bool ordered, bool reliable) Parameters Type Name Description String label The data channel name. Boolean ordered Indicates whether data channel messages are ordered (see Ordered ). Boolean reliable Indicates whether data channel messages are reliably delivered (see Reliable ). Returns Type Description Task < DataChannel > Returns a task which completes once the data channel is created. | Improve this Doc View Source AddDataChannelAsync(UInt16, String, Boolean, Boolean) Add a new out-of-band data channel with the given ID. A data channel is negotiated out-of-band when the peers agree on an identifier by any mean not known to WebRTC, and both open a data channel with that ID. The WebRTC will match the incoming and outgoing pipes by this ID to allow sending and receiving through that channel. This requires some external mechanism to agree on an available identifier not otherwise taken by another channel, and also requires to ensure that both peers explicitly open that channel. Declaration public Task<DataChannel> AddDataChannelAsync(ushort id, string label, bool ordered, bool reliable) Parameters Type Name Description UInt16 id The unique data channel identifier to use. String label The data channel name. Boolean ordered Indicates whether data channel messages are ordered (see Ordered ). Boolean reliable Indicates whether data channel messages are reliably delivered (see Reliable ). Returns Type Description Task < DataChannel > Returns a task which completes once the data channel is created. | Improve this Doc View Source AddIceCandidate(String, Int32, String) Inform the WebRTC peer connection of a newly received ICE candidate. Declaration public void AddIceCandidate(string sdpMid, int sdpMlineindex, string candidate) Parameters Type Name Description String sdpMid Int32 sdpMlineindex String candidate | Improve this Doc View Source AddLocalAudioTrackAsync() Add to the current connection an audio track from a local audio capture device (microphone). Declaration public Task AddLocalAudioTrackAsync() Returns Type Description Task Asynchronous task completed once the device is capturing and the track is added. Remarks On UWP this requires the \"microphone\" capability. See https://docs.microsoft.com/en-us/windows/uwp/packaging/app-capability-declarations for more details. | Improve this Doc View Source AddLocalVideoTrackAsync(PeerConnection.VideoCaptureDevice, Boolean) Add to the current connection a video track from a local video capture device (webcam). Declaration public Task AddLocalVideoTrackAsync(PeerConnection.VideoCaptureDevice device = default(PeerConnection.VideoCaptureDevice), bool enableMrc = false) Parameters Type Name Description PeerConnection.VideoCaptureDevice device Optional device to use, defaults to first available one. Boolean enableMrc Returns Type Description Task Asynchronous task completed once the device is capturing and the track is added. Remarks On UWP this requires the \"webcam\" capability. See https://docs.microsoft.com/en-us/windows/uwp/packaging/app-capability-declarations for more details. | Improve this Doc View Source Close() Close the peer connection and destroy the underlying native resources. Declaration public void Close() | Improve this Doc View Source CreateAnswer() Create an SDP answer message to a previously-received offer, to accept a connection. Declaration public bool CreateAnswer() Returns Type Description Boolean true if the offer was created successfully. | Improve this Doc View Source CreateOffer() Create an SDP offer message as an attempt to establish a connection. Declaration public bool CreateOffer() Returns Type Description Boolean true if the offer was created successfully. | Improve this Doc View Source Dispose() Dispose of native resources by closing the peer connection. Declaration public void Dispose() Remarks This is equivalent to Close() . | Improve this Doc View Source GetVideoCaptureDevicesAsync() Get the list of available video capture devices. Declaration public static Task<List<PeerConnection.VideoCaptureDevice>> GetVideoCaptureDevicesAsync() Returns Type Description Task < List < PeerConnection.VideoCaptureDevice >> The list of available video capture devices. | Improve this Doc View Source InitializeAsync(CancellationToken) Initialize the current peer connection object asynchronously. Declaration public Task InitializeAsync(CancellationToken token = null) Parameters Type Name Description CancellationToken token Optional cancellation token for the initialize task. This is only used if the singleton task was created by this call, and not a prior call. Returns Type Description Task The singleton task used to initialize the underlying native peer connection. Remarks This method is multi-thread safe, and will always return the same task object from the first call to it until the peer connection object is deinitialized. This allows multiple callers to all execute some action following the initialization, without the need to force a single caller and to synchronize with it. MemCpy(Void*, Void*, UInt64) Unsafe utility to copy a contiguous block of memory. This is equivalent to the C function memcpy() , and is provided for optimization purpose only. Declaration public static void MemCpy(void *dst, void *src, ulong size) Parameters Type Name Description Void * dst Pointer to the beginning of the destination buffer data is copied to. Void * src Pointer to the beginning of the source buffer data is copied from. UInt64 size Size of the memory block, in bytes. MemCpyStride(Void*, Int32, Void*, Int32, Int32, Int32) Unsafe utility to copy a memory block with stride. This utility loops over the rows of the input memory block, and copy them to the output memory block, then increment the read and write pointers by the source and destination strides, respectively. For each row, exactly elem_size bytes are copied, even if the row stride is higher. The extra bytes in the destination buffer past the row size until the row stride are left untouched. This is equivalent to the following pseudo-code: for (int row = 0; row < elem_count; ++row) { memcpy(dst, src, elem_size); dst += dst_stride; src += src_stride; } Declaration public static void MemCpyStride(void *dst, int dst_stride, void *src, int src_stride, int elem_size, int elem_count) Parameters Type Name Description Void * dst Pointer to the beginning of the destination buffer data is copied to. Int32 dst_stride Stride in bytes of the destination rows. This must be greater than or equal to the row size elem_size . Void * src Pointer to the beginning of the source buffer data is copied from. Int32 src_stride Stride in bytes of the source rows. This must be greater than or equal to the row size elem_size . Int32 elem_size Size of each row, in bytes. Int32 elem_count Total number of rows to copy. | Improve this Doc View Source RemoveLocalAudioTrack() Remove from the current connection the local audio track added with AddLocalAudioTrackAsync() . Declaration public void RemoveLocalAudioTrack() | Improve this Doc View Source RemoveLocalVideoTrack() Remove from the current connection the local video track added with AddLocalAudioTrackAsync() . Declaration public void RemoveLocalVideoTrack() | Improve this Doc View Source SetRemoteDescription(String, String) Pass the given SDP description received from the remote peer via signaling to the underlying WebRTC implementation, which will parse and use it. This must be called by the signaler when receiving a message. Declaration public void SetRemoteDescription(string type, string sdp) Parameters Type Name Description String type The type of SDP message (\"offer\", \"answer\", \"ice\") String sdp The content of the SDP message Events | Improve this Doc View Source ARGBLocalVideoFrameReady Event that occurs when a video frame from a local track has been produced locally and is available for render. Declaration public event ARGBVideoFrameDelegate ARGBLocalVideoFrameReady Event Type Type Description ARGBVideoFrameDelegate | Improve this Doc View Source ARGBRemoteVideoFrameReady Event that occurs when a video frame from a remote peer has been received and is available for render. Declaration public event ARGBVideoFrameDelegate ARGBRemoteVideoFrameReady Event Type Type Description ARGBVideoFrameDelegate | Improve this Doc View Source Connected Event fired when a connection is established. Declaration public event Action Connected Event Type Type Description Action | Improve this Doc View Source I420LocalVideoFrameReady Event that occurs when a video frame from a local track has been produced locally and is available for render. Declaration public event I420VideoFrameDelegate I420LocalVideoFrameReady Event Type Type Description I420VideoFrameDelegate | Improve this Doc View Source I420RemoteVideoFrameReady Event that occurs when a video frame from a remote peer has been received and is available for render. Declaration public event I420VideoFrameDelegate I420RemoteVideoFrameReady Event Type Type Description I420VideoFrameDelegate | Improve this Doc View Source IceCandidateReadytoSend Event that occurs when a local ICE candidate is ready to be transmitted. Declaration public event PeerConnection.IceCandidateReadytoSendDelegate IceCandidateReadytoSend Event Type Type Description PeerConnection.IceCandidateReadytoSendDelegate | Improve this Doc View Source LocalSdpReadytoSend Event that occurs when a local SDP message is ready to be transmitted. Declaration public event PeerConnection.LocalSdpReadyToSendDelegate LocalSdpReadytoSend Event Type Type Description PeerConnection.LocalSdpReadyToSendDelegate | Improve this Doc View Source RenegotiationNeeded Event that occurs when a renegotiation of the session is needed. This generally occurs as a result of adding or removing tracks, and the user should call CreateOffer() to actually start a renegotiation. Declaration public event Action RenegotiationNeeded Event Type Type Description Action | Improve this Doc View Source TrackAdded Event that occurs when a remote track is added to the current connection. Declaration public event Action TrackAdded Event Type Type Description Action | Improve this Doc View Source TrackRemoved Event that occurs when a remote track is removed from the current connection. Declaration public event Action TrackRemoved Event Type Type Description Action"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDevice.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDevice.html",
    "title": "Struct PeerConnection.VideoCaptureDevice | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.VideoCaptureDevice Identifier for a video capture device. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoCaptureDevice Fields | Improve this Doc View Source id Unique device identifier. Declaration public string id Field Value Type Description String | Improve this Doc View Source name Friendly device name. Declaration public string name Field Value Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate.html",
    "title": "Delegate PeerConnection.IceCandidateReadytoSendDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceCandidateReadytoSendDelegate Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceCandidateReadytoSendDelegate(string candidate, int sdpMlineindex, string sdpMid); Parameters Type Name Description String candidate Int32 sdpMlineindex String sdpMid"
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoFrameQueue.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoFrameQueue.html",
    "title": "Interface IVideoFrameQueue | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoFrameQueue Interface for a queue of video frames. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoFrameQueue Properties | Improve this Doc View Source DequeuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video sink consumes some video frames, typically to render them. Declaration float DequeuedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source DroppedFramesPerSecond Get the number of frames dropped per seconds. This is generally an average statistics representing how many frames were enqueued by a video source but not dequeued fast enough by a video sink, meaning the video sink renders at a slower framerate than the source can produce. Declaration float DroppedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source QueuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video source produces some video frames. Declaration float QueuedFramesPerSecond { get; } Property Value Type Description Single"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStoppedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStoppedEvent.html",
    "title": "Class VideoStreamStoppedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoStreamStoppedEvent Unity event corresponding to an on-going video stream being stopped. Inheritance Object VideoStreamStoppedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class VideoStreamStoppedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStartedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStartedEvent.html",
    "title": "Class VideoStreamStartedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoStreamStartedEvent Unity event corresponding to a new video stream being started. Inheritance Object VideoStreamStartedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class VideoStreamStartedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.RemoteVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.RemoteVideoSource.html",
    "title": "Class RemoteVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class RemoteVideoSource This component represents a remote video source added as a video track to an existing WebRTC peer connection by a remote peer and received locally. The video track can optionally be displayed locally with a MediaPlayer . Inheritance Object VideoSource RemoteVideoSource Inherited Members VideoSource.FrameQueue VideoSource.VideoStreamStarted VideoSource.VideoStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class RemoteVideoSource : VideoSource Fields AutoPlayOnAdded Automatically play the remote video track when it is added. This is equivalent to manually calling Play() when the peer connection is initialized. Declaration public bool AutoPlayOnAdded Field Value Type Description Boolean See Also Play() Stop() PeerConnection Peer connection this remote video source is extracted from. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection Properties IsPlaying Is the video source currently playing? The concept of playing is described in the Play() function. Declaration public bool IsPlaying { get; } Property Value Type Description Boolean See Also Play() Stop() Methods Awake() Implementation of MonoBehaviour.Awake which registers some handlers with the peer connection to listen to its OnInitialized and OnShutdown events. Declaration protected void Awake() OnDestroy() Implementation of MonoBehaviour.OnDestroy which unregisters all listeners from the peer connection. Declaration protected void OnDestroy() Play() Manually start playback of the remote video feed by registering some listeners to the peer connection and starting to enqueue video frames as they become ready. Because the WebRTC implementation uses a push model, calling Play() does not necessarily start producing frames immediately. Instead, this starts listening for incoming frames from the remote peer. When a track is actually added by the remote peer and received locally, the VideoStreamStarted event is fired, and soon after frames will start being available for rendering in the internal frame queue. Note that this event may be fired before Play() is called, in which case frames are produced immediately. If AutoPlayOnAdded is true then this is called automatically as soon as the peer connection is initialized. Declaration public void Play() Remarks This is only valid while the peer connection is initialized, that is after the OnInitialized event was fired. See Also Stop() IsPlaying Stop() Stop playback of the remote video feed and unregister the handler listening to remote video frames. Note that this is independent of whether or not a remote track is actually present. In particular this does not fire the VideoStreamStopped , which corresponds to a track being made available to the local peer by the remote peer. Declaration public void Stop() See Also Play() IsPlaying Update() Implementation of MonoBehaviour.Update to execute from the current Unity main thread any background work enqueued from free-threaded callbacks. Declaration protected void Update()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.RemoteAudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.RemoteAudioSource.html",
    "title": "Class RemoteAudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Class RemoteAudioSource This component represents a remote audio source added as an audio track to an existing WebRTC peer connection by a remote peer and received locally. The audio track can optionally be displayed locally with a MediaPlayer . Inheritance Object AudioSource RemoteAudioSource Inherited Members AudioSource.AudioStreamStarted AudioSource.AudioStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class RemoteAudioSource : AudioSource Fields AudioTrackAdded Event triggered when a remote audio track is added remotely and received locally. Declaration public AudioTrackAddedEvent AudioTrackAdded Field Value Type Description AudioTrackAddedEvent AudioTrackRemoved Event triggered when a remote audio track is removed remotely and stops begin received locally. Declaration public AudioTrackRemovedEvent AudioTrackRemoved Field Value Type Description AudioTrackRemovedEvent AutoPlayOnAdded Automatically play the remote audio track when it is added. Declaration public bool AutoPlayOnAdded Field Value Type Description Boolean PeerConnection Peer connection this remote audio source is extracted from. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection Methods Awake() Declaration protected void Awake() OnDestroy() Declaration protected void OnDestroy()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.MediaPlayer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.MediaPlayer.html",
    "title": "Class MediaPlayer | MixedReality-WebRTC Documentation",
    "keywords": "Class MediaPlayer Play video frames received from a WebRTC video track. Inheritance Object MediaPlayer Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class MediaPlayer : MonoBehaviour Remarks This component writes to the attached Material , via the attached Renderer . Fields AudioSource Declaration public AudioSource AudioSource Field Value Type Description AudioSource EnableStatistics Declaration public bool EnableStatistics Field Value Type Description Boolean FrameLoadStatHolder A textmesh onto which frame load stat data will be written Declaration public TextMesh FrameLoadStatHolder Field Value Type Description TextMesh Remarks This is how fast the frames are given from the underlying implementation FramePresentStatHolder A textmesh onto which frame present stat data will be written Declaration public TextMesh FramePresentStatHolder Field Value Type Description TextMesh Remarks This is how fast we render frames to the display FrameQueue The frame queue from which frames will be rendered. Declaration public VideoFrameQueue<I420VideoFrameStorage> FrameQueue Field Value Type Description VideoFrameQueue < I420VideoFrameStorage > FrameSkipStatHolder A textmesh into which frame skip stat dta will be written Declaration public TextMesh FrameSkipStatHolder Field Value Type Description TextMesh Remarks This is how often we skip presenting an underlying frame MaxVideoFramerate Declaration public float MaxVideoFramerate Field Value Type Description Single VideoSource Declaration public VideoSource VideoSource Field Value Type Description VideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSource.html",
    "title": "Class LocalVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoSource This component represents a local video source added as a video track to an existing WebRTC peer connection and sent to the remote peer. The video track can optionally be displayed locally with a MediaPlayer . Inheritance Object VideoSource LocalVideoSource Inherited Members VideoSource.FrameQueue VideoSource.VideoStreamStarted VideoSource.VideoStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class LocalVideoSource : VideoSource Fields AutoAddTrack Automatically register as a video track when the peer connection is ready. Declaration public bool AutoAddTrack Field Value Type Description Boolean Remarks If this is false then the user needs to manually call Microsoft.MixedReality.WebRTC.PeerConnection.AddLocalVideoTrackAsync(Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDevice,bool) to add a video track to the peer connection and start sending video data to the remote peer. AutoStartCapture Automatically start local video capture when this component is enabled. Declaration public bool AutoStartCapture Field Value Type Description Boolean EnableMixedRealityCapture Enable Mixed Reality Capture (MRC) if available on the local device. This option has no effect on devices not supporting MRC. Declaration public bool EnableMixedRealityCapture Field Value Type Description Boolean PeerConnection Peer connection this local video source will add a video track to. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection PreferredVideoCodec Name of the preferred video codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredVideoCodec Field Value Type Description String Methods Awake() Declaration protected void Awake() OnDestroy() Declaration protected void OnDestroy() OnDisable() Callback when the Unity component is disabled. This is the proper way to disable the video source and get it to stop video capture. Declaration protected void OnDisable() OnEnable() Callback when the Unity component is enabled. This is the proper way to enable the video source and get it to start video capture and enqueue video frames. Declaration protected void OnEnable()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC.Unity.Editor | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC.Unity.Editor Classes LocalVideoSourceEditor Inspector editor for LocalVideoSource . Allows displaying some error message when Mixed Reality Capture is enabled but XR is not, the later corresponding to a non-exclusive app (2D slate) where MRC is not available."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.LocalVideoSourceEditor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.LocalVideoSourceEditor.html",
    "title": "Class LocalVideoSourceEditor | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoSourceEditor Inspector editor for LocalVideoSource . Allows displaying some error message when Mixed Reality Capture is enabled but XR is not, the later corresponding to a non-exclusive app (2D slate) where MRC is not available. Inheritance Object LocalVideoSourceEditor Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class LocalVideoSourceEditor : UnityEditor.Editor Methods OnInspectorGUI() Override implementation of Editor.OnInspectorGUI to draw the inspector GUI for the currently selected LocalVideoSource . Declaration public override void OnInspectorGUI()"
  },
  "manual/unity-components.html": {
    "href": "manual/unity-components.html",
    "title": "Unity components | MixedReality-WebRTC Documentation",
    "keywords": "Unity components The Unity components provide some idiomatic wrapping over the C# MixedReality-WebRTC library. Component Description PeerConnection Encapsulates a single peer-to-peer connection to a remote peer Signaler Abstract base class of components which manage the signaling messages to allow the peer connection to establish NodeDssSignaler Simple testing / debugging Signaler implementation component based on node-dss VideoSource Component providing a hook to the local and remote video tracks of a peer connection VideoTrackPlayer Component bridging a Unity MeshRenderer with a video track from a VideoSource component Components by feature area Connection The most important component is Microsoft.MixedReality.WebRTC.Unity.PeerConnection which encapsulate the connection to a single remote peer. The peer connection works in coordination with a Signaler component, for example Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler , which handles the message transport for the session establishment. Video The entry point for video tracks is the VideoSource component, which is associated with a given PeerConnection and handles its video-related signals. The video source in turn makes use of one or two VideoTrackPlayer to render the content of the local and remote video tracks if needed, although this is optional. Video frames are provided by the underlying WebRTC.PeerConnection to the VideoSource via its frame events. The video source then fills a VideoFrameQueue for each video track player associated with it. The player use that shared queue to read back the frame and display it using a custom shader. Audio Todo... List of components Microsoft.MixedReality.WebRTC.Unity.PeerConnection This component abstracts a WebRTC peer connection and encapsulates the lower level Microsoft.MixedReality.WebRTC.PeerConnection object from the C# library. This is the main entry point for establishing a connection. It contains the list of ICE servers ( Interactive Connectivity Establishment ) used to punch through NAT, as well as some key events like OnInitialized and OnShutdown which mark the beginning and end of the connection from the point of view of the user. Microsoft.MixedReality.WebRTC.Unity.Signaler This abstract base component is used by the peer connection to establish a connection with a remote peer. The peer connection needs one concrete implementation derived from this class to be specified in its Signaler property. Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler THIS SHOULD NOT BE USED FOR PRODUCTION. This components is used for debugging and testing as a concrete implementation of a Signaler component. It is based on the node-dss protocol and NodeJS service. It is very simple and helps developers starting, but lacks many features. Microsoft.MixedReality.WebRTC.Unity.VideoSource This component controls sending the local video track through the peer connection, and handles receiving a remote video track. For the local video, it controls whether or not a track is added to the WebRTC stream, and optionally provides a VideoTrackPlayer with frames to render that local video. For the remote video, it controls whether or not to handle the received feed and send it to a VideoTrackPlayer for rendering. Note that the local peer cannot control whether or not the remote peer sends a remote track; it can only ignore it if not interested (and ideally should probably tell the remote peer that it should stop sending it, although this is application specific logic). Microsoft.MixedReality.WebRTC.Unity.VideoTrackPlayer This components bridges a raw video frame feed from a VideoSource to a Unity MeshRenderer for display. The component can limit the framerate of the video playback, and optionally display some statistics about it. The associated MeshRenderer on the same GameObject typically uses a YUVFeedMaterial to display the YUV-encoded feed uploaded to the main texture of that material by the video track player component."
  },
  "manual/signaling.html": {
    "href": "manual/signaling.html",
    "title": "Signaling | MixedReality-WebRTC Documentation",
    "keywords": "Signaling (coming soon!)"
  },
  "manual/installation.html": {
    "href": "manual/installation.html",
    "title": "Installation | MixedReality-WebRTC Documentation",
    "keywords": "Installation (coming soon!)"
  },
  "manual/helloworld-unity.html": {
    "href": "manual/helloworld-unity.html",
    "title": "Hello, Unity world! | MixedReality-WebRTC Documentation",
    "keywords": "Hello, Unity world! In this tutorial we will create a simple Unity application based on the MixedReality-WebRTC Unity integration . Creating a new Unity project Importing MixedReality-WebRTC Creating a peer connection Creating a signaler Adding local video Adding remote video Establishing a connection"
  },
  "manual/helloworld-unity-importwebrtc.html": {
    "href": "manual/helloworld-unity-importwebrtc.html",
    "title": "Importing MixedReality-WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Importing MixedReality-WebRTC In order to use the Unity integration, the following components are required: C++ library : Microsoft.MixedReality.WebRTC.Native.dll C# library : Microsoft.MixedReality.WebRTC.dll Unity integration Copying the libraries The C++ and C# libraries are currently not available as pre-built binaries, so need to be compiled from sources. See the installation for details. Once compiled, the libraries are available in a sub-folder of the bin/ folder of the MixedReality-WebRTC project. Note that the copy is automatically done by a build script when compiling via the provided Visual Studio projects . The steps below are only required if the libraries are compiled in another way. In that case you can skip to the next step to Configure the import settings . The C# library Microsoft.MixedReality.WebRTC.dll is a .NET Standard 2.0 library. This means it is compatible with all CPU architectures. This is often referred to as \"AnyCPU\", and the C# library is therefore available from bin\\AnyCPU\\Debug or bin\\AnyCPU\\Release depending on the build configuration which was compiled. In doubt you can use the Release configuration, which provides better performance. This module needs to be copied somewhere into the Assets\\Plugins\\ folder of the Unity project (if that folder doesn't exist you can create it). On Windows this can be done via the command line with xcopy , assuming that the MixedReality-WebRTC project is located in D:\\mr-webrtc : cd /D D:\\testproj xcopy D:/mr-webrtc/bin/AnyCPU/Release/Microsoft.MixedReality.WebRTC.dll Assets/Plugins/ For the C++ library Microsoft.MixedReality.WebRTC.Native.dll things are a bit more complex. The C++ code is compiled for a particular platform and architecture, in addition of the Debug or Release build config, and the correct variant needs to be used. On Windows, the Unity Editor needs a 64-bit Desktop variant; it is available from the bin\\Win32\\x64\\Release folder, and should be copied to the Assets\\Plugins\\Win32\\x86_64\\ folder. cd /D D:\\testproj xcopy D:/mr-webrtc/bin/Win32/x64/Release/Microsoft.MixedReality.WebRTC.Native.dll Assets/Plugins/Win32/x86_64/ Configuring the import settings When building the Unity application for a given platform, another variant may be required. In order for the C# library to be truly platform-independent, the name of all C++ library variants is the same. This allows the C# code to reference the C++ library with the same DllImport attribute path . But this also means that Unity needs to know which copy is associated with which build variant, to be able to deploy the correct one. This is done by configuring the platform associated with a DLL in the import settings in the Unity inspector: By selecting: Any Platform except WSAPlayer , the DLL will be used by Unity on all platforms except when deploying for UWP. WSAPlayer is the name Unity uses for its UWP standalone player. CPU equal to x86_64 , Unity will only deploy that DLL when deploying on a 64-bit Intel architecture. This way, multiple variants of the same-named Microsoft.MixedReality.WebRTC.Native.dll can co-exist in different sub-folders of Assets/Plugins/ and Unity will deploy and use the correct variant on each platform. For Windows Desktop , the C++ library variants are: Path Any Platform Exclude Platforms CPU OS Example use Assets/Plugins/Win32/x86 yes - x86 Windows 32-bit Windows Desktop application Assets/Plugins/Win32/x86_64 yes - x86_64 Windows 64-bit Windows Desktop application, including the Unity Editor on Windows For Windows UWP , the C++ library variants are: Path Any Platform Include Platforms SDK CPU Example use Assets/Plugins/UWP/x86 no +WSAPlayer UWP X86 Microsoft HoloLens Assets/Plugins/UWP/x86_64 no +WSAPlayer UWP X64 64-bit UWP Desktop app on Windows Assets/Plugins/UWP/ARM no +WSAPlayer UWP ARM HoloLens 2 (compatibility) Assets/Plugins/UWP/ARM64 no +WSAPlayer UWP ARM64* HoloLens 2 *ARM64 is only available on Unity 2019.1+ If all variants are installed, the resulting hierarchy should look like this: Assets +- Plugins +- Win32 | +- x86 | | +- Microsoft.MixedReality.WebRTC.Native.dll | +- x86_64 | +- Microsoft.MixedReality.WebRTC.Native.dll +- UWP +- x86 | +- Microsoft.MixedReality.WebRTC.Native.dll +- x86_64 | +- Microsoft.MixedReality.WebRTC.Native.dll +- ARM | +- Microsoft.MixedReality.WebRTC.Native.dll +- ARM64 +- Microsoft.MixedReality.WebRTC.Native.dll Importing the Unity integration In order to import the Unity integration into your new Unity project, simply copy the libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Microsoft.MixedReality.WebRTC.Unity and libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Microsoft.MixedReality.WebRTC.Unity.Editor folders into the Assets folder of your project. The former provides the integration itself, while the later contains some helpers for the Unity Editor. Those helpers are only needed in the Editor, and not when the application is deployed at runtime. After Unity finished processing the new files, the Project window should look like this:"
  },
  "manual/helloworld-unity-createproject.html": {
    "href": "manual/helloworld-unity-createproject.html",
    "title": "Creating a new Unity project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a new Unity project Go to the Unity archive page and download the latest 2018.3.x version or 2019.1.x version. Other versions might work, but are not officially supported. Create a new Unity project by selecting the 3D Template and choosing a project name and location. Unity will create several folders at that location, the most important of which is the Assets folder. This contains all source files for the project, and this is where the MixedReality-WebRTC Unity integration will be installed."
  },
  "manual/helloworld-cs.html": {
    "href": "manual/helloworld-cs.html",
    "title": "Hello, C# World! | MixedReality-WebRTC Documentation",
    "keywords": "Hello, C# World!"
  },
  "manual/building.html": {
    "href": "manual/building.html",
    "title": "Building from sources | MixedReality-WebRTC Documentation",
    "keywords": "Building from sources This document describes how to build the entire project from sources, including the so-called Core dependencies: The low-level WebRTC C++ implementation webrtc.lib from Google. The UWP WinRT wrapper Org.WebRtc.winmd from the Microsoft WebRTC UWP team. The dependencies require some heavy setup and take time to compile, therefore it is strongly recommended to use the prebuilt binaries shipped as NuGet packages instead of trying to build those from source: NuGet Package Architecture webrtc.lib Org.WebRtc.winmd Microsoft.MixedReality.WebRTC.Core.Native.Desktop x86, x64 ✔ ❌ Microsoft.MixedReality.WebRTC.Core.Native.UWP.x86 x86 ✔ ✔ Microsoft.MixedReality.WebRTC.Core.Native.UWP.x64 x64 ✔ ✔ Microsoft.MixedReality.WebRTC.Core.Native.UWP.ARM ARM (32-bit) ✔ ✔ Microsoft.MixedReality.WebRTC.Core.Native.UWP (*) x86, x64, ARM ✔ ✔ (*) This is a meta-package referencing all the architecture-dependent ones for UWP for convenience. In general most users will want to follow the steps in the readme instead of the ones below if there is no need to modify the input dependencies. Prerequisites General Python 2.7 must be installed as the default interpreter. That is, python --version must return a Python version equal to 2.7. It is strongly recommended to get a patch version >= 15, that is Python 2.7.15+ , as some users reported to the WebRTC UWP team some spurious failures with earlier versions. Python 3.x does not work and should not be the default interpreter. A recent version of Perl is needed for some builds. On Windows you can install for example Strawberry Perl , or any other equivalent distribution you want. Core WebRTC Core WebRTC refers to the C++ implementation of WebRTC maintained by Google and used by this project, whose source repository is https://webrtc.googlesource.com/src . Visual Studio 2017 is required to compile the core WebRTC implementation from Google. Having the MSVC v141 toolchain installed inside another version of Visual Studio is unfortunately not enough (see this issue ), the actual IDE needs to be installed for the detection script to work. Selecting the C++ Workload alone is enough. If compiling for ARM or ARM64 architecture though, check the Visual C++ compilers and libraries for ARM(64) optional individual component. The Windows SDK 10.0.17134 (also called 1803, or April 2018) is required to compile the Google WebRTC core implementation ( archive download ). As mentioned on the README of WebRTC UWP, the Debugging Tools for Windows are required: When installing the SDK, include the feature Debugging Tools for Windows which is required by the preparation scripts. Note that the SDK installed as part of Visual Studio does not include this feature. If the SDK is already installed, this optional feature can be added with Add or Remove Programs > Windows Software Development Kit - Windows 10.0.x > Modify > Select Change then Next button > Check Debugging Tools for Windows . Core WebRTC UWP wrappers The UWP wrappers refer to the set of wrappers and other UWP-specific additional code made available by the WebRTC UWP team (Microsoft) on top of the core implementation, to allow access to the core WebRTC API. The Windows SDK 10.0.17763 (also called 1809, or October 2018) is required to compile the UWP wrappers provided by the WebRTC UWP team ( archive download ), with the Debugging Tools for Windows as above. The UWP wrappers also require the v141 platform toolset for UWP, either from the Universal Windows Platform development workload in Visual Studio 2017, or from the optional component C++ (v141) Universal Windows Platform tools in Visual Studio 2019 . The UWP wrappers use C++/WinRT, so the C++/WinRT Visual Studio extension must be installed from the marketplace. MixedReality-WebRTC C++ library The MSVC v142 - VS 2019 C++ x64/x86 build tools toolchain is required to build the C++17 library of MixedReality-WebRTC. This is installed by default with the Desktop development with C++ workload on Visual Studio 2019. Note - Currently due to CI limitations some projects are downgraded to VS 2017, but will be reverted to VS 2019 eventually (see #14). Unity integration The Unity integration has been tested on Unity 2018.3.x and 2019.1.x . Versions earlier than 2018.3.x may work but are not officially supported. Build steps Check out the repository and its dependencies git clone --recursive https://github.com/microsoft/MixedReality-WebRTC.git Note that this may take some time (> 5 minutes) due to the large number of submodules in the WebRTC UWP SDK repository this repository depends on. Build the WebRTC UWP SDK libraries Using the build script In order to simplify building, a PowerShell build script is available. The prerequisites still need to be installed manually before running it. To use the script, simply run for example: cd tools/build/ build.ps1 -BuildConfig Debug -BuildArch x64 -BuildPlatform Win32 Note - Currently the build script assumes it runs from tools/build/ only. It will fail if invoked from another directory. Valid parameter values are: BuildConfig : Debug | Release BuildArch : x86 | x64 | ARM | ARM64 BuildPlatform : Win32 | UWP Note - ARM and ARM64 are only valid for the UWP platform. Note - ARM64 is not yet available (see #13 ). The manual steps are details below and can be skipped if running the build script. Manually The WebRTC UWP project has specific requirements . In particular it needs Python 2.7.15+ installed as default , that is calling python from a shell without specifying a path launches that Python 2.7.15+ version. Note - Currently the Azure hosted agents with VS 2017 have Python 2.7.14 installed, but this is discouraged by the WebRTC UWP team as some spurious build errors might occur. The new VS 2019 build agents have Python 2.7.16 installed. Note - Currently the libyuv external dependency is incorrectly compiled with Clang instead of MSVC on ARM builds. This was an attempt to benefit from inline assembly, but this produces link errors (see this issue ). Until this is fixed, a patch is available under tools\\patches\\libyuv_win_msvc_157.patch which is applied by build.ps1 but needs to be applied manually if build.ps1 is not used. More generally all patches under tools\\patches need to be manually applied . For Windows 10 Desktop support (also called \"Win32\"): Open the WebRtc.Win32.sln Visual Studio solution located in external\\webrtc-uwp-sdk\\webrtc\\windows\\solution\\ In the menu bar, select the relevant solution platform and solution configuration. For the Unity editor, the x64 binaries are required. Build the WebRtc.Win32.Native.Builder project alone , which generates some files needed by some of the other projects in the solution, by right-clicking on that project > Build . The other projects are samples and are not needed. For UWP support: Open the WebRtc.Universal.sln Visual Studio solution located in external\\webrtc-uwp-sdk\\webrtc\\windows\\solution\\ In the menu bar, select the relevant solution platform and solution configuration. For HoloLens, the x86 binaries are required. For HoloLens 2, the ARM binaries are required (ARM64 is not supported yet, see #13 ). Build first the WebRtc.UWP.Native.Builder project alone , which generates some files needed by some of the other projects in the solution, by right-clicking on that project > Build Next build the Org.WebRtc and Org.WebRtc.WrapperGlue projects. The other projects samples and are not needed. Build the MixedReality-WebRTC libraries Open the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the repository. Build the solution with F7 or Build > Build Solution On successful build, the binaries will be generated in a sub-directory under bin/ , and the relevant DLLs will be copied by a post-build script to libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Plugins\\ for Unity to consume them. The Microsoft.MixedReality.WebRTC.sln Visual Studio solution contains several projects: The native C++ library, which can be compiled: for Windows Desktop with the Microsoft.MixedReality.WebRTC.Native.Win32 project for UWP with the Microsoft.MixedReality.WebRTC.Native.UWP project The C# library project Microsoft.MixedReality.WebRTC A C# unit tests project Microsoft.MixedReality.WebRTC.Tests A UWP C# sample app project Microsoft.MixedReality.WebRTC.TestAppUWP based on WPF and XAML Optionally test the installation Test the install by e.g. opening the Unity project at libs\\Microsoft.MixedReality.WebRTC.Unity , loading the Assets\\Microsoft.MixedReality.WebRTC\\Unity.Examples\\SimpleVideoChat scene and pressing Play . After a few seconds (depending on the machine) the left media player should display the video feed from the local webcam. The Unity console should also display a message about the WebRTC library being initialized successfully. See the Hello, Unity World! tutorial for more details."
  },
  "index.html": {
    "href": "index.html",
    "title": "Index | MixedReality-WebRTC Documentation",
    "keywords": "MixedReality-WebRTC 0.1 documentation (Coming soon!) User Manual Getting started Download Prerequisites Building C# library Feature Overview Signaling Audio Streaming Rendering Video Streaming Rendering Unity integration Feature Overview Tutorial Samples Signaling Audio Streaming Rendering Video Streaming Rendering API documentation C# library PeerConnection The PeerConnection object is the API entry point to establish a remote connection. ISignaler The signaler interface allows using different signaling implementations. VideoFrameQueue<T> The video frame queue bridges a video source and a video sink. Unity integration PeerConnection The PeerConnection component builds on the same-named library class to expose a remote peer connection. Signaler The abstract Signaler component is the base class for signaling implementations. LocalVideoSource The LocalVideoSource component provides access to the local webcam for local rendering and remote streaming."
  },
  "manual/peerconnection.html": {
    "href": "manual/peerconnection.html",
    "title": "Peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Peer connection (coming soon!)"
  },
  "manual/introduction.html": {
    "href": "manual/introduction.html",
    "title": "Introduction | MixedReality-WebRTC Documentation",
    "keywords": "Introduction The MixedReality-WebRTC project is a collection of components to help mixed reality app developers to integrate peer-to-peer audio and video streaming into their application and improve their collaborative experience. These components are based on the WebRTC protocol for Real-Time Communication (RTC), which is supported by most modern web browsers. Project scope The MixedReality-WebRTC project focuses on features which actively contribute to enhance collaborative experiences in mixed reality apps. Although the WebRTC technology can be used for many different video and audio streaming application -- and in fact the MixedReality-WebRTC project itself can be used in any application, even non-mixed-reality ones -- the project scope limits the features the maintaining team will spend its development resources on. This does not mean that other features are not welcome; as an open-source project, we welcome any contribution. However we will dedicate more of our own resources to those contributions within the project scope."
  },
  "manual/helloworld-unity-signaler.html": {
    "href": "manual/helloworld-unity-signaler.html",
    "title": "Creating a signaler | MixedReality-WebRTC Documentation",
    "keywords": "Creating a signaler The WebRTC standard specifies how a peer-to-peer connection can be established using the Session Description Protocol (SDP) , but does not enforce a particular signaling solution to discover and select a remote peer, and to send to and receive from it the SDP messages necessary to establish that connection. MixedReality-WebRTC offers a built-in solution in the form of the NodeDssSignaler component, but also allows any other custom implementation to be used. For this tutorial, we will use the NodeDssSignaler component for simplicity. Caution NodeDssSignaler is very simple and useful for getting started quickly and debugging, but it is worth noting that this is not a production-quality solution and should not be used in production . In particular, this component offers no security whatsoever , all communications happening in clear text over HTTP, and no authentication , as all it requires to connect with a remote peer is to know its identifier. Do not be fooled by the fact that WebRTC supports encryption, as it would be very easy for an attacker to bypass it by compromising the signaler. Remember that any security solution is no better that its weakest link, and NodeDssSignaler is that link. It must be replaced with a secure solution when moving to production. The NodeDssSignaler component uses a Node.js server with a simple polling model where both peers are constantly checking if a message is available. The server implementation is called node-dss and is freely available on GitHub. Install and run node-dss The node-dss repository has instructions on how to install and run the server , which essentially boil down to installing Node.js, downloading the code, and running: set DEBUG=dss* npm install npm start This opens a console window which will output all requests received from all peers. Leave that console window open and the node-dss server running for the following, and go back to the Unity editor. Creating a NodeDssSignaler The NodeDssSignaler component can be added to the existing GameObject , or to a new one. There is no fundamental difference, and this is mostly a matter of taste. For this tutorial we will create a separate game object to separate it from the peer connection in the Hierarchy window. Create a new GameObject with a NodeDssSignaler component: In the Hierarchy window , select Create > Create empty to add a new GameObject to the scene. In the Inspector window , select Add Component > MixedReality-WebRTC > NodeDssSignaler to add a NodeDssSignaler component to that new object. Optionally rename the game object to something easy to remember like \"MySignaler\" to easily find it in the Hierarchy window. By default the NodeDssSignaler component is configured to connect to a node-dss server running locally on the developper machine at http://127.0.0.1:3000/ and poll the server every 500 milliseconds to query for available messages. Connecting the signaler Now that a signaling solution is available, the last step is to assign the PeerConnection.Signaler property of our peer connection to that implementation: In the Hierarchy window, select the game object with the peer connection component In the Inspector window, find the Signaler property and click on the circle to its right to bring the asset selection window From the Scene tab, select the game object with the NodeDssSignaler component on it. The signaler object should now appear in the Inspector window of the peer connection game object. At that point the peer connection is fully configured and ready to be used. However audio and video tracks are not added automatically, so there is little use for that peer connection. Next we will look at connecting a local webcam and microphone to provide some video and audio track to send through to the peer connection."
  },
  "manual/helloworld-unity-remotevideo.html": {
    "href": "manual/helloworld-unity-remotevideo.html",
    "title": "Adding remote video | MixedReality-WebRTC Documentation",
    "keywords": "Adding remote video Unlike local video, the remote video track is controlled by the remote peer who will decide if and when to add the track to the connection. On the receiver side, the only thing to decide is whether or not to handle that track. The RemoteVideoSource Unity component is used to expose a remote video track. And similarly to the LocalVideoSource , it can be added to a MediaPlayer to render the content of the video feed. Adding a remote video source Like we did for the local video feed, we create a new game object with a RemoteVideoSource component: In the Hierarchy window, select Create > Create Empty . Rename it to something memorable like \"RemoteMediaPlayer\". Go to the Transform component and increase the scale to (5,5,1) . In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > RemoteVideoSource . Like the local video source, this component needs to know which peer connection to use. Once again, use the asset selection window to assign our peer connection to the Peer Connection property. We note immediately that the remote video source is more simple than the local video source. Aside from the PeerConnection property we already assigned, the only other property is the Auto Play On Added boolean. This property instructs the component to immediately start playing back the video feed when the remote track is added to the connection and received locally. If this property is false , then the user needs to manually call Play() to start listening to incoming remote video frames. Adding a remote media player This is again similar to the local video source: the remote video source only exposes a video frame queue which gets populated using the video frames coming from the remote peer, but the component does not do any rendering by itself. Instead we can use again a MediaPlayer to render those frames. So let's create a new game object for it: Add a Mesh Filter component, a Mesh Renderer component, and a Media Player component. In the Mesh Filter component, set the Mesh property to the built-in Unity Quad mesh. In the Mesh Renderer component, expand the Materials array and set the first material Element 0 to the YUVFeedMaterial material located in the Assets/Microsoft.MixedReality.WebRTC.Unity/Materials folder. In the Media Player component, set the Video Source property to the remote video source component previously created. The Inspector window should now look like this: At this point we need to fix a small issue : both the local and remote media player objects are at the same location. Change the Position of the local one to move it slightly to the left at (-3,0,0) . Change the Position of the remote one to move it slightly to the right at (3,0,0) . After that we should be able to see in the Game window our two pink squares representing the local and remote videos:"
  },
  "manual/helloworld-unity-peerconnection.html": {
    "href": "manual/helloworld-unity-peerconnection.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection From this point we start building the scene. Because the MixedReality-WebRTC components are installed, and because we work now almost exclusively inside Unity, for brievety we will use the term component to designate a Unity component, that is a class deriving from MonoBehaviour . Create a new GameObject with a PeerConnection component: In the Hierarchy window , select Create > Create Empty to add a new GameObject to the scene. In the Inspector window , select Add Component > MixedReality-WebRTC > PeerConnection to add a PeerConnection component to that new object. At the top of the Inspector window, rename the newly-created game object to something memorable like \"MyPeerConnection\". You can also rename this object in the Hierarchy window directly (for example by pressing F2 when selected). The PeerConnection component provided by the Unity integration of MixedReality-WebRTC has various settings to configure its behaviour. For the moment you can leave the default values. The component has one required property however, the PeerConnection.Signaler property, which it uses to establish a connection. This property must point to another component deriving from the base Microsoft.MixedReality.WebRTC.Unity.Signaler component. For the moment it points to nothing, so Unity shows \"None\"."
  },
  "manual/helloworld-unity-localvideo.html": {
    "href": "manual/helloworld-unity-localvideo.html",
    "title": "Adding local video | MixedReality-WebRTC Documentation",
    "keywords": "Adding local video There are two different aspects covered by the concept of local video : Capturing some video feed from a local camera to send it to the remote peer Displaying locally the captured video feed Both are optional, although the second one alone simply corresponds to capturing a displaying a local webcam and doesn't require WebRTC. So we generally want the first one in a scenario where an application needs WebRTC. The second one is application-dependent, and even within one given application can be toggled ON and OFF by the user. Both cases however are covered by the LocalVideoSource component. This components serves as a bridge between a local video capture device (camera), the peer connection, and an optional video player to render the video feed locally. Adding a local video source For clarity we will create a new game object and add a LocalVideoSource component. It may sound superfluous at the moment to create a new game object, as we could add the local video source to the same game object already owning the peer connection component, but this will prove more clear and easy to manipulate later. In the Hierarchy window, select Create > Create Empty . In the Inspector window, rename the newly-created game object to something memorable like \"LocalMediaPlayer\". Press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > LocalVideoSource . This component needs to know which peer connection to use. Once again, use the asset selection window to assign our peer connection to the Peer Connection property. The local video source component contains several interesting properties: The Auto Start Capture property instructs the component to open the video capture device (webcam) automatically as soon as possible. This enables starting local video playback even before the peer connection is established. The Enable Mixed Reality Capture property tells the component it should attempt to open the video capture device with MRC enabled, if supported. The Auto Add Track property allows automatically adding a video track to the peer connection and start sending the video feed to the remote peer once the connection is established. If not checked, the user has to manually call some method to add that track. These are good defaults values to start, and we will leave them as is. Adding a media player We said before that the LocalVideoSource component covers both sending the video feed to the remote peer and displaying it locally. This is partially incorrect. The local video source plugs into the peer connection and the video capture device, and exposes some C# event to access the video frames produced by that video device. But it does not do any rendering itself. In order to render the video frames of the local video capture device, MixedReality-WebRTC offers a simple MediaPlayer component which uses a Unity Texture2D object and renders the video frames to it. This texture is then applied to the material of a Renderer component to be displayed in Unity on a mesh. Let's add a MediaPlayer component on our game object: In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > MediaPlayer This time however Unity will not create the component, and instead display a somewhat complex error message: What the message means is that the MediaPlayer component requires a Renderer component on the same game object, and Unity lists all possible implementation of a renderer (all classes deriving from Renderer ). Although all renderers might work, in our case the most simple is to add a MeshRenderer component. If you are familiar with Unity, you also know that the renderer needs a source mesh in the form of a MeshFilter component. So for each component, in the Inspector window, press the Add Component button at the bottom of the window, and select successively and in order: Mesh > MeshFilter Mesh > MeshRenderer MixedReality-WebRTC > MediaPlayer After that, set the component properties as follow: In the Mesh Filter component, set the Mesh property to the built-in Unity Quad mesh. This is a simple square mesh on which the texture containing the video feed will be applied. The built-in Quad mesh size is quite small for rendering a video, so go to the Transform component and increase the scale to (5,5,1) . In the Mesh Renderer component, expand the Materials array and set the first material Element 0 to the YUVFeedMaterial material located in the Assets/Microsoft.MixedReality.WebRTC.Unity/Materials folder. This instructs Unity to use that special material and its associated shader to render the video texture on the quad mesh. More on that later. In the Media Player component, set the Video Source property to the local video source component previously added to the same game object. This instructs the media player to connect to the local video source for retrieving the video frames that it will copy to the video texture for rendering. This should result in a setup looking like this: And the Game view should display a pink square, which materializes the quad mesh: A word on the YUVFeedMaterial material here. The video frames coming from the local video source are encoded using the I420 format. Unity on the other hand, and more specifically the GPU it abstracts, generally don't support directly rendering I420-encoded textures. So the YUVFeedMaterial material is using a custom shader called YUVFeedShader (Unlit) to load the I420-encoded video frame from the video texture, and convert it to ARGB on the fly before rendering the quad. This GPU-based conversion is very efficient and avoids any software processing on the CPU before uploading the video texture to the GPU. This is how LocalVideoSource is able to directly copy the I420-encoded video frames coming from the WebRTC core implementation into a texture without further processing, and MediaPlayer is able to render them on a quad mesh. Test the local video At this point the local audio source and the media player are configured to open the local video capture device (webcam) of the local machine the Unity Editor is running on, and display the video feed to that quad mesh in the scene. Press the Play button in the Unity Editor. After a few seconds (depending on the device) the video should appear over the quad mesh."
  },
  "manual/helloworld-unity-connection.html": {
    "href": "manual/helloworld-unity-connection.html",
    "title": "Establishing a connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a connection Now that we have a both local and remote video players, we can attempt to establish a connection with a remote peer. There are a few requirements for this: We need 2 instances of the application running at the same time. Unfortunately the Unity Editor cannot be opened twice with the same project. As a workaround, we can build and deploy the app on a device, even locally on the developer machine. Alternatively, we can use a second computer running another instance of the Unity Editor with an exact copy of this Unity project. The later is easier because we can still modify the project. The NodeDssSignaler component needs to be configured to know which remote peer to expect. This is due to the fact that this is a simple, easy, and not production-ready solution which does not offer any functionality to discover and select a remote peer. Instead it uses strings to identify the two peers. We can chose any two different strings. Configuring the NodeDssSignaler The NodeDssSignaler has a Remote Peer Id property which contains the string identifying the remote peer to connect with. This should be filled with the identifier of the remote peer. The easiest way to obtain this identifier is to press Play and wait for the local signaler to start polling our node-dss server. If the server was started with the DEBUG=*dss environment variable set, it will output for each web request a message containing the identifier of the peer. Download and install Node.js from the official website . Clone the node-dss repository : git clone https://github.com/bengreenier/node-dss.git Configure and run it: cd node-dss set DEBUG=dss* npm install npm start The node-dss server should start in a new shell window and wait for incoming connections. At this point we can press Play in the Unity Editor to start polling the node-dss server, and retrieve from the shell terminal its identifier string: This string needs to be pasted into the Remote Peer Id property of the remote peer on the other machine. Repeat the process on the remote machine and paste the result on the Remote Peer Id property of the local machine. Warning This step is critical, and there is no safeguard. If any of the two signalers doesn't have the correct value for the identifier of the remote peer then the peer connection will not be established. Testing the remote connection"
  },
  "manual/gettingstarted.html": {
    "href": "manual/gettingstarted.html",
    "title": "Getting started | MixedReality-WebRTC Documentation",
    "keywords": "Getting started The MixedReality-WebRTC project is comprised of several components: A C++ library to integrate into a native C++ application A C# library to integrate into a C# application A Unity integration to help integrate into an existing Unity application Not all components are required for all use cases, but each component builds upon the previous one. This means that for use in a C++ application only the C++ library needs to be installed. But the Unity integration will require installing also the C# and C++ libraries. A note on terminology: in this documentation the term component refers to one of the libraries mentioned above. This has no relation with a Unity component , which is a C# class deriving from MonoBehaviour. In this chapter we discuss: Installation : How to install the various components. Hello, C# world! : Your first C# application based on the C# library. Hello, Unity world! : Your first Unity application based on the Unity integration."
  },
  "manual/avplayback.html": {
    "href": "manual/avplayback.html",
    "title": "Audio and video playback | MixedReality-WebRTC Documentation",
    "keywords": "Audio and video playback (coming soon!)"
  },
  "api/Microsoft.MixedReality.WebRTC.I420VideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420VideoFrameStorage.html",
    "title": "Class I420VideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Class I420VideoFrameStorage Storage for a video frame encoded in I420 format. Inheritance Object I420VideoFrameStorage Implements IVideoFrameStorage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class I420VideoFrameStorage : object, IVideoFrameStorage Properties | Improve this Doc View Source Buffer Declaration public byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Declaration public ulong Capacity { get; set; } Property Value Type Description UInt64 | Improve this Doc View Source Height Declaration public uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Declaration public uint Width { get; set; } Property Value Type Description UInt32 Implements IVideoFrameStorage"
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrame.html",
    "title": "Struct I420AVideoFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct I420AVideoFrame Single video frame encoded in I420A format (triplanar YUV + alpha, 18 bits per pixel). See e.g. https://wiki.videolan.org/YUV/#I420 for details. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct I420AVideoFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. Fields | Improve this Doc View Source dataA Optional pointer to the alpha plane buffer, if any, or null if the frame has no alpha plane. Declaration public IntPtr dataA Field Value Type Description IntPtr | Improve this Doc View Source dataU Pointer to the U plane buffer. Declaration public IntPtr dataU Field Value Type Description IntPtr | Improve this Doc View Source dataV Pointer to the V plane buffer. Declaration public IntPtr dataV Field Value Type Description IntPtr | Improve this Doc View Source dataY Pointer to the Y plane buffer. Declaration public IntPtr dataY Field Value Type Description IntPtr | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source strideA Stride in bytes between rows of the A plane, if present. Declaration public int strideA Field Value Type Description Int32 | Improve this Doc View Source strideU Stride in bytes between rows of the U plane. Declaration public int strideU Field Value Type Description Int32 | Improve this Doc View Source strideV Stride in bytes between rows of the V plane. Declaration public int strideV Field Value Type Description Int32 | Improve this Doc View Source strideY Stride in bytes between rows of the Y plane. Declaration public int strideY Field Value Type Description Int32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32 Methods | Improve this Doc View Source CopyTo(Byte[]) Copy the frame content to a Byte [] buffer as a contiguous block of memory containing the Y, U, and V planes one after another, and the alpha plane at the end if present. Declaration public void CopyTo(byte[] buffer) Parameters Type Name Description Byte [] buffer The destination buffer to copy the frame to."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.html",
    "title": "Enum DataChannel.ChannelState | MixedReality-WebRTC Documentation",
    "keywords": "Enum DataChannel.ChannelState Connecting state of a data channel, when adding it to a peer connection or removing it from a peer connection. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum ChannelState : int Fields Name Description Closed The data channel reached end of life and can be destroyed. It cannot be re-connected; instead a new data channel must be created. Closing The data channel is being closed, and is not available anymore for data exchange. Connecting The data channel has just been created, and negotiating is underway to establish a track between the peers. Open The data channel is open and ready to send and receive messages."
  },
  "api/Microsoft.MixedReality.WebRTC.AudioFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioFrameDelegate.html",
    "title": "Delegate AudioFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate AudioFrameDelegate Delegate used for events when an audio frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void AudioFrameDelegate(AudioFrame frame); Parameters Type Name Description AudioFrame frame The newly available audio frame."
  },
  "api/Microsoft.MixedReality.WebRTC.ARGBVideoFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.ARGBVideoFrameDelegate.html",
    "title": "Delegate ARGBVideoFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate ARGBVideoFrameDelegate Delegate used for events when an ARGB-encoded video frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void ARGBVideoFrameDelegate(ARGBVideoFrame frame); Parameters Type Name Description ARGBVideoFrame frame The newly available ARGB-encoded video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC.Unity | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC.Unity Classes AudioSource Base class for audio sources plugging into the internal peer connection API to expose a single audio stream to a renderer ( MediaPlayer or custom). AudioStreamStartedEvent Unity event corresponding to a new audio stream being started. AudioStreamStoppedEvent Unity event corresponding to an on-going audio stream being stopped. AudioTrackAddedEvent Unity event corresponding to a new audio track added to the current connection by the remote peer. AudioTrackRemovedEvent Unity event corresponding to an existing audio track removed from the current connection by the remote peer. LocalAudioSource This component represents a local audio source added as an audio track to an existing WebRTC peer connection and sent to the remote peer. The audio track can optionally be rendered locally with a MediaPlayer . LocalVideoSource This component represents a local video source added as a video track to an existing WebRTC peer connection and sent to the remote peer. The video track can optionally be displayed locally with a MediaPlayer . MediaPlayer Play video frames received from a WebRTC video track. NodeDssSignaler Simple signaler for debug and testing. This is based on https://github.com/bengreenier/node-dss and SHOULD NOT BE USED FOR PRODUCTION. PeerConnection High-level wrapper for Unity WebRTC functionalities. This is the API entry point for establishing a connection with a remote peer. RemoteAudioSource This component represents a remote audio source added as an audio track to an existing WebRTC peer connection by a remote peer and received locally. The audio track can optionally be displayed locally with a MediaPlayer . RemoteVideoSource This component represents a remote video source added as a video track to an existing WebRTC peer connection by a remote peer and received locally. The video track can optionally be displayed locally with a MediaPlayer . Signaler Base class for WebRTC signaling implementations in Unity. VideoSource Base class for video sources plugging into the internal peer connection API to expose a single video stream to a renderer ( MediaPlayer or custom). VideoStreamStartedEvent Unity event corresponding to a new video stream being started. VideoStreamStoppedEvent Unity event corresponding to an on-going video stream being stopped. WebRTCErrorEvent A UnityEvent that represents a WebRTC error event. Structs ConfigurableIceServer Represents an Ice server in a simple way that allows configuration from the unity inspector Enums IceType Enumeration of the different types of ICE servers."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.WebRTCErrorEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.WebRTCErrorEvent.html",
    "title": "Class WebRTCErrorEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class WebRTCErrorEvent A UnityEvent that represents a WebRTC error event. Inheritance Object WebRTCErrorEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class WebRTCErrorEvent : UnityEvent<string>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalAudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalAudioSource.html",
    "title": "Class LocalAudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalAudioSource This component represents a local audio source added as an audio track to an existing WebRTC peer connection and sent to the remote peer. The audio track can optionally be rendered locally with a MediaPlayer . Inheritance Object AudioSource LocalAudioSource Inherited Members AudioSource.AudioStreamStarted AudioSource.AudioStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class LocalAudioSource : AudioSource Fields AutoAddTrack Automatically register as an audio track when the peer connection is ready. Declaration public bool AutoAddTrack Field Value Type Description Boolean AutoStartCapture Automatically start local audio capture when this component is enabled. Declaration public bool AutoStartCapture Field Value Type Description Boolean PeerConnection Peer connection this local audio source will add an audio track to. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection PreferredAudioCodec Name of the preferred audio codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredAudioCodec Field Value Type Description String Methods Awake() Declaration protected void Awake() OnDestroy() Declaration protected void OnDestroy() OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.IceType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.IceType.html",
    "title": "Enum IceType | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceType Enumeration of the different types of ICE servers. Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum IceType Fields Name Description None Indicates there is no ICE information Stun Indicates ICE information is of type STUN Turn Indicates ICE information is of type TURN"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStartedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStartedEvent.html",
    "title": "Class AudioStreamStartedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioStreamStartedEvent Unity event corresponding to a new audio stream being started. Inheritance Object AudioStreamStartedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class AudioStreamStartedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioSource.html",
    "title": "Class AudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioSource Base class for audio sources plugging into the internal peer connection API to expose a single audio stream to a renderer ( MediaPlayer or custom). Inheritance Object AudioSource LocalAudioSource RemoteAudioSource Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class AudioSource : MonoBehaviour Fields AudioStreamStarted Declaration public AudioStreamStartedEvent AudioStreamStarted Field Value Type Description AudioStreamStartedEvent AudioStreamStopped Declaration public AudioStreamStoppedEvent AudioStreamStopped Field Value Type Description AudioStreamStoppedEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate.html",
    "title": "Delegate PeerConnection.LocalSdpReadyToSendDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.LocalSdpReadyToSendDelegate Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void LocalSdpReadyToSendDelegate(string type, string sdp); Parameters Type Name Description String type String sdp"
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.html",
    "title": "Class DataChannel | MixedReality-WebRTC Documentation",
    "keywords": "Class DataChannel Encapsulates a data channel of a peer connection. A data channel is a pipe allowing to send and receive arbitrary data to the remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted). Exact security guarantees are provided by the underlying WebRTC core implementation and the WebRTC standard itself. https://tools.ietf.org/wg/rtcweb/ An instance of DataChannel is created by calling AddDataChannelAsync(UInt16, String, Boolean, Boolean) or one of its variants. DataChannel cannot be instantiated directly. Inheritance Object DataChannel Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class DataChannel : IDisposable Properties | Improve this Doc View Source ID Declaration public int ID { get; } Property Value Type Description Int32 The unique identifier of the data channel in the current connection. | Improve this Doc View Source Label Declaration public string Label { get; } Property Value Type Description String The data channel name in the current connection. | Improve this Doc View Source Ordered Indicates whether the data channel messages are ordered or not. Ordered messages are delivered in the order they are sent, at the cost of delaying later messages delivery to the application (via MessageReceived ) when internally arriving out of order. Declaration public bool Ordered { get; } Property Value Type Description Boolean true if messages are ordered. See Also Reliable | Improve this Doc View Source PeerConnection Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection The PeerConnection object this data channel was created from. | Improve this Doc View Source Reliable Indicates whether the data channel messages are reliably delivered. Reliable messages are guaranteed to be delivered as long as the connection is not dropped. Unreliable messages may be silently dropped for whatever reason, and the implementation will not try to detect this nor resend them. Declaration public bool Reliable { get; } Property Value Type Description Boolean true if messages are reliable. See Also Ordered | Improve this Doc View Source State The channel connection state represents the connection status when creating or closing the data channel. Changes to this state are notified via the StateChanged event. Declaration public DataChannel.ChannelState State { get; } Property Value Type Description DataChannel.ChannelState The channel connection state. See Also StateChanged Methods | Improve this Doc View Source Dispose() Remove the data channel from the peer connection and destroy it. Declaration public void Dispose() | Improve this Doc View Source Finalize() Declaration protected void Finalize() | Improve this Doc View Source SendMessage(Byte[]) Send a message through the data channel. Declaration public void SendMessage(byte[] message) Parameters Type Name Description Byte [] message The message to send to the remote peer. See Also InitializeAsync(CancellationToken) Initialized Events | Improve this Doc View Source MessageReceived Event fires when a message is received through the data channel. Declaration public event Action<byte[]> MessageReceived Event Type Type Description Action < Byte []> See Also SendMessage(Byte[]) | Improve this Doc View Source StateChanged Event fired when the data channel state changes, as reported by State . Declaration public event Action StateChanged Event Type Type Description Action See Also State"
  },
  "api/Microsoft.MixedReality.WebRTC.AudioFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioFrame.html",
    "title": "Struct AudioFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct AudioFrame Single audio frame. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct AudioFrame"
  },
  "api/Microsoft.MixedReality.WebRTC.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC Classes DataChannel Encapsulates a data channel of a peer connection. A data channel is a pipe allowing to send and receive arbitrary data to the remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted). Exact security guarantees are provided by the underlying WebRTC core implementation and the WebRTC standard itself. https://tools.ietf.org/wg/rtcweb/ An instance of DataChannel is created by calling AddDataChannelAsync(UInt16, String, Boolean, Boolean) or one of its variants. DataChannel cannot be instantiated directly. I420VideoFrameStorage Storage for a video frame encoded in I420 format. MovingAverage Utility to manage a moving average of a time series. PeerConnection The WebRTC peer connection object is the entry point to using WebRTC. SignalerMessage Data that makes up a signaler message TaskExtensions Collection of extension methods for Task . VideoFrameQueue<T> Small queue of video frames received from a source and pending delivery to a sink. Used as temporary buffer between the WebRTC callback (push model) and the video player rendering (pull model). This also handles dropping frames when the source is faster than the sink, by limiting the maximum queue length. Structs ARGBVideoFrame Single video frame encoded in ARGB interleaved format (32 bits per pixel). AudioFrame Single audio frame. I420AVideoFrame Single video frame encoded in I420A format (triplanar YUV + alpha, 18 bits per pixel). See e.g. https://wiki.videolan.org/YUV/#I420 for details. PeerConnection.VideoCaptureDevice Identifier for a video capture device. Interfaces ISignaler Interface for a signaling implementation, which provides PeerConnection with the capability to send and receive messages in order to establish a connection with a remote peer. IVideoFrameQueue Interface for a queue of video frames. IVideoFrameStorage Interface for a storage of a single video frame. Enums DataChannel.ChannelState Connecting state of a data channel, when adding it to a peer connection or removing it from a peer connection. SignalerMessage.WireMessageType Possible message types as-serialized on the wire Delegates ARGBVideoFrameDelegate Delegate used for events when an ARGB-encoded video frame has been produced and is ready for consumption. AudioFrameDelegate Delegate used for events when an audio frame has been produced and is ready for consumption. I420VideoFrameDelegate Delegate used for events when an I420-encoded video frame has been produced and is ready for consumption. PeerConnection.IceCandidateReadytoSendDelegate PeerConnection.LocalSdpReadyToSendDelegate PeerConnection.VideoCaptureDeviceEnumCallback PeerConnection.VideoCaptureDeviceEnumCompletedCallback"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoFrameQueue-1.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoFrameQueue-1.html",
    "title": "Class VideoFrameQueue<T> | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoFrameQueue<T> Small queue of video frames received from a source and pending delivery to a sink. Used as temporary buffer between the WebRTC callback (push model) and the video player rendering (pull model). This also handles dropping frames when the source is faster than the sink, by limiting the maximum queue length. Inheritance Object VideoFrameQueue<T> Implements IVideoFrameQueue Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class VideoFrameQueue<T> : object, IVideoFrameQueue where T : class, IVideoFrameStorage, new() Type Parameters Name Description T The type of video frame storage Constructors | Improve this Doc View Source VideoFrameQueue(Int32) Create a new queue with a maximum frame length. Declaration public VideoFrameQueue(int maxQueueLength) Parameters Type Name Description Int32 maxQueueLength Maxmimum number of frames to enqueue before starting to drop incoming frames Properties | Improve this Doc View Source DequeuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video sink consumes some video frames, typically to render them. Declaration public float DequeuedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source DroppedFramesPerSecond Get the number of frames dropped per seconds. This is generally an average statistics representing how many frames were enqueued by a video source but not dequeued fast enough by a video sink, meaning the video sink renders at a slower framerate than the source can produce. Declaration public float DroppedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source QueuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video source produces some video frames. Declaration public float QueuedFramesPerSecond { get; } Property Value Type Description Single Methods | Improve this Doc View Source Enqueue(ARGBVideoFrame) Try to enqueue a new video frame encoded in raw ARGB format. If the internal queue reached its maximum capacity, do nothing and drop the frame. Declaration public bool Enqueue(ARGBVideoFrame frame) Parameters Type Name Description ARGBVideoFrame frame The video frame to enqueue Returns Type Description Boolean Return true if the frame was enqueued successfully, or false if it was dropped Remarks This should only be used if the queue has storage for a compatible video frame encoding. | Improve this Doc View Source Enqueue(I420AVideoFrame) Enqueue a new video frame encoded in I420 format. If the internal queue reached its maximum capacity, do nothing and drop the frame. Declaration public bool Enqueue(I420AVideoFrame frame) Parameters Type Name Description I420AVideoFrame frame The video frame to enqueue Returns Type Description Boolean Return true if the frame was enqueued successfully, or false if it was dropped Remarks This should only be used if the queue has storage for a compatible video frame encoding. | Improve this Doc View Source RecycleStorage(T) Recycle a frame storage, putting it back into the internal pool for later reuse. This prevents deallocation and reallocation of a frame, and decreases pressure on the garbage collector. Declaration public void RecycleStorage(T frame) Parameters Type Name Description T frame The unused frame storage to recycle for a later new frame | Improve this Doc View Source TryDequeue(out T) Try to dequeue a video frame, usually to be consumed by a video sink (video player). Declaration public bool TryDequeue(out T frame) Parameters Type Name Description T frame On success, returns the dequeued frame. Returns Type Description Boolean Return true on success or false if the queue is empty. Implements IVideoFrameQueue"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioTrackAddedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioTrackAddedEvent.html",
    "title": "Class AudioTrackAddedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioTrackAddedEvent Unity event corresponding to a new audio track added to the current connection by the remote peer. Inheritance Object AudioTrackAddedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class AudioTrackAddedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStoppedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStoppedEvent.html",
    "title": "Class AudioStreamStoppedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioStreamStoppedEvent Unity event corresponding to an on-going audio stream being stopped. Inheritance Object AudioStreamStoppedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class AudioStreamStoppedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoSource.html",
    "title": "Class VideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoSource Base class for video sources plugging into the internal peer connection API to expose a single video stream to a renderer ( MediaPlayer or custom). Inheritance Object VideoSource LocalVideoSource RemoteVideoSource Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class VideoSource : MonoBehaviour Fields FrameQueue Frame queue holding the pending frames enqueued by the video source itself, which a video renderer needs to read and display. Declaration public VideoFrameQueue<I420VideoFrameStorage> FrameQueue Field Value Type Description VideoFrameQueue < I420VideoFrameStorage > VideoStreamStarted Event invoked from the main Unity thread when the video stream starts. This means that video frames are available and the renderer should start polling. Declaration public VideoStreamStartedEvent VideoStreamStarted Field Value Type Description VideoStreamStartedEvent VideoStreamStopped Event invoked from the main Unity thread when the video stream stops. This means that the video frame queue is not populated anymore, though some frames may still be present in it that may be rendered. Declaration public VideoStreamStoppedEvent VideoStreamStopped Field Value Type Description VideoStreamStoppedEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.html",
    "title": "Class Signaler | MixedReality-WebRTC Documentation",
    "keywords": "Class Signaler Base class for WebRTC signaling implementations in Unity. Inheritance Object Signaler NodeDssSignaler Implements ISignaler Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class Signaler : MonoBehaviour, ISignaler Fields _nativePeer Native PeerConnection object from the underlying WebRTC C# library, available once the peer has been initialized and the signaler is attached to it. Declaration protected PeerConnection _nativePeer Field Value Type Description PeerConnection Properties PeerConnection The PeerConnection this signaler is attached to, or null if not attached yet to any connection. This is updated automatically by the peer connection once it finished initializing. Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection Methods OnIceCandiateReadyToSend(String, Int32, String) Callback fired when an ICE candidate message has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected abstract void OnIceCandiateReadyToSend(string candidate, int sdpMlineIndex, string sdpMid) Parameters Type Name Description String candidate Int32 sdpMlineIndex String sdpMid OnPeerInitialized(PeerConnection) Callback fired from the PeerConnection when it finished initializing, to subscribe to signaling-related events. Declaration public void OnPeerInitialized(PeerConnection peer) Parameters Type Name Description PeerConnection peer The peer connection to attach to OnPeerUninitializing(PeerConnection) Callback fired from the PeerConnection before it starts uninitializing itself and disposing of the underlying implementation object. Declaration public void OnPeerUninitializing(PeerConnection peer) Parameters Type Name Description PeerConnection peer The peer connection about to be deinitialized OnSdpAnswerReadyToSend(String) Callback fired when a local SDP answer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected abstract void OnSdpAnswerReadyToSend(string answer) Parameters Type Name Description String answer The SDP answer message to send. OnSdpOfferReadyToSend(String) Callback fired when a local SDP offer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected abstract void OnSdpOfferReadyToSend(string offer) Parameters Type Name Description String offer The SDP offer message to send. SendMessageAsync(SignalerMessage) Asynchronously send a signaling message to the remote peer. Declaration public abstract Task SendMessageAsync(SignalerMessage message) Parameters Type Name Description SignalerMessage message The signaling message to send to the remote peer. Returns Type Description Task A Task object completed once the message has been sent, but not necessarily delivered. Update() Declaration protected virtual void Update() Events OnConnect Declaration public event Action OnConnect Event Type Type Description Action OnDisconnect Declaration public event Action OnDisconnect Event Type Type Description Action OnFailure Declaration public event Action<Exception> OnFailure Event Type Type Description Action < Exception > OnMessage Declaration public event Action<SignalerMessage> OnMessage Event Type Type Description Action < SignalerMessage > Implements ISignaler"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.PeerConnection.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.PeerConnection.html",
    "title": "Class PeerConnection | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection High-level wrapper for Unity WebRTC functionalities. This is the API entry point for establishing a connection with a remote peer. Inheritance Object PeerConnection Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class PeerConnection : MonoBehaviour Fields AutoInitializeOnStart Flag to initialize the peer connection on MonoBehaviour.Start() . Declaration public bool AutoInitializeOnStart Field Value Type Description Boolean AutoLogErrorsToUnityConsole Flag to log all errors to the Unity console automatically. Declaration public bool AutoLogErrorsToUnityConsole Field Value Type Description Boolean IceCredential Optional credential for the ICE servers. Declaration public string IceCredential Field Value Type Description String IceServers Set of ICE servers the WebRTC library will use to try to establish a connection. Declaration public List<ConfigurableIceServer> IceServers Field Value Type Description List < ConfigurableIceServer > IceUsername Optional username for the ICE servers. Declaration public string IceUsername Field Value Type Description String OnError Event that occurs when a WebRTC error occurs Declaration public WebRTCErrorEvent OnError Field Value Type Description WebRTCErrorEvent OnInitialized Event fired after the peer connection is initialized and ready for use. Declaration public UnityEvent OnInitialized Field Value Type Description UnityEvent OnShutdown Event fired after the peer connection is shut down and cannot be used anymore. Declaration public UnityEvent OnShutdown Field Value Type Description UnityEvent Signaler Signaler to use to establish the connection. Declaration public Signaler Signaler Field Value Type Description Signaler Properties Peer Retrieves the underlying peer connection object once initialized. Declaration public PeerConnection Peer { get; } Property Value Type Description PeerConnection Remarks If OnInitialized has not fired, this will be null . Methods GetVideoCaptureDevicesAsync() Enumerate the video capture devices available as a WebRTC local video feed source. Declaration public static Task<List<PeerConnection.VideoCaptureDevice>> GetVideoCaptureDevicesAsync() Returns Type Description Task < List < PeerConnection.VideoCaptureDevice >> The list of local video capture devices available to WebRTC. InitializeAsync(CancellationToken) Initialize the underlying WebRTC libraries Declaration public Task InitializeAsync(CancellationToken token = default(CancellationToken)) Parameters Type Name Description CancellationToken token Returns Type Description Task Remarks This function is asynchronous, to monitor it's status bind a handler to OnInitialized and OnError Uninitialize() Uninitialize the underlying WebRTC library, effectively cleaning up the allocated peer connection. Declaration public void Uninitialize() Remarks Peer will be null afterward."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler.html",
    "title": "Class NodeDssSignaler | MixedReality-WebRTC Documentation",
    "keywords": "Class NodeDssSignaler Simple signaler for debug and testing. This is based on https://github.com/bengreenier/node-dss and SHOULD NOT BE USED FOR PRODUCTION. Inheritance Object Signaler NodeDssSignaler Implements ISignaler Inherited Members Signaler.PeerConnection Signaler.OnConnect Signaler.OnDisconnect Signaler.OnMessage Signaler.OnFailure Signaler._nativePeer Signaler.OnPeerInitialized(PeerConnection) Signaler.OnPeerUninitializing(PeerConnection) Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class NodeDssSignaler : Signaler, ISignaler Fields AutoLogErrors Automatically log all errors to the Unity console. Declaration public bool AutoLogErrors Field Value Type Description Boolean HttpServerAddress The https://github.com/bengreenier/node-dss HTTP service address to connect to Declaration public string HttpServerAddress Field Value Type Description String PollTimeMs The interval (in ms) that the server is polled at Declaration public float PollTimeMs Field Value Type Description Single RemotePeerId Unique identifier of the remote peer. Declaration public string RemotePeerId Field Value Type Description String Properties LocalPeerId Unique identifier of the local peer. Declaration public string LocalPeerId { get; set; } Property Value Type Description String Methods OnIceCandiateReadyToSend(String, Int32, String) Callback fired when an ICE candidate message has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected override void OnIceCandiateReadyToSend(string candidate, int sdpMlineIndex, string sdpMid) Parameters Type Name Description String candidate Int32 sdpMlineIndex String sdpMid Overrides Signaler.OnIceCandiateReadyToSend(String, Int32, String) OnSdpAnswerReadyToSend(String) Callback fired when a local SDP answer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected override void OnSdpAnswerReadyToSend(string answer) Parameters Type Name Description String answer Overrides Signaler.OnSdpAnswerReadyToSend(String) OnSdpOfferReadyToSend(String) Callback fired when a local SDP offer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected override void OnSdpOfferReadyToSend(string offer) Parameters Type Name Description String offer Overrides Signaler.OnSdpOfferReadyToSend(String) SendMessageAsync(SignalerMessage) Asynchronously send a signaling message to the remote peer. Declaration public override Task SendMessageAsync(SignalerMessage message) Parameters Type Name Description SignalerMessage message The signaling message to send to the remote peer. Returns Type Description Task A Task object completed once the message has been sent, but not necessarily delivered. Overrides Signaler.SendMessageAsync(SignalerMessage) Update() Unity Engine Update() hook Declaration protected override void Update() Overrides Signaler.Update() Remarks https://docs.unity3d.com/ScriptReference/MonoBehaviour.Update.html Implements ISignaler"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.ConfigurableIceServer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.ConfigurableIceServer.html",
    "title": "Struct ConfigurableIceServer | MixedReality-WebRTC Documentation",
    "keywords": "Struct ConfigurableIceServer Represents an Ice server in a simple way that allows configuration from the unity inspector Inherited Members ValueType.Equals(Object) ValueType.GetHashCode() Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetType() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public struct ConfigurableIceServer Fields Type The type of the server Declaration public IceType Type Field Value Type Description IceType Uri The unqualified uri of the server Declaration public string Uri Field Value Type Description String Remarks You should not prefix this with \"stun:\" or \"turn:\" Methods ToString() Convert the server to the representation the underlying libraries use Declaration public override string ToString() Returns Type Description String stringified server information Overrides ValueType.ToString()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioTrackRemovedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioTrackRemovedEvent.html",
    "title": "Class AudioTrackRemovedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioTrackRemovedEvent Unity event corresponding to an existing audio track removed from the current connection by the remote peer. Inheritance Object AudioTrackRemovedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class AudioTrackRemovedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDeviceEnumCompletedCallback.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDeviceEnumCompletedCallback.html",
    "title": "Delegate PeerConnection.VideoCaptureDeviceEnumCompletedCallback | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.VideoCaptureDeviceEnumCompletedCallback Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void VideoCaptureDeviceEnumCompletedCallback();"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDeviceEnumCallback.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDeviceEnumCallback.html",
    "title": "Delegate PeerConnection.VideoCaptureDeviceEnumCallback | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.VideoCaptureDeviceEnumCallback Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void VideoCaptureDeviceEnumCallback(string id, string name); Parameters Type Name Description String id String name"
  },
  "api/Microsoft.MixedReality.WebRTC.MovingAverage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.MovingAverage.html",
    "title": "Class MovingAverage | MixedReality-WebRTC Documentation",
    "keywords": "Class MovingAverage Utility to manage a moving average of a time series. Inheritance Object MovingAverage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class MovingAverage : object Constructors | Improve this Doc View Source MovingAverage(Int32) Create a new moving average with a given window size. Declaration public MovingAverage(int capacity) Parameters Type Name Description Int32 capacity The capacity of the sample window. Properties | Improve this Doc View Source Average Average value of the samples. Declaration public float Average { get; } Property Value Type Description Single | Improve this Doc View Source Capacity Number of samples in the moving average window. Declaration public int Capacity { get; } Property Value Type Description Int32 Methods | Improve this Doc View Source Push(Single) Push a new sample and recalculate the current average. Declaration public void Push(float value) Parameters Type Name Description Single value The new value to add."
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoFrameStorage.html",
    "title": "Interface IVideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoFrameStorage Interface for a storage of a single video frame. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw storage buffer of capacity Capacity . Declaration byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Storage capacity, in bytes. Declaration ulong Capacity { get; set; } Property Value Type Description UInt64 | Improve this Doc View Source Height Frame height, in pixels. Declaration uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration uint Width { get; set; } Property Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.ISignaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.ISignaler.html",
    "title": "Interface ISignaler | MixedReality-WebRTC Documentation",
    "keywords": "Interface ISignaler Interface for a signaling implementation, which provides PeerConnection with the capability to send and receive messages in order to establish a connection with a remote peer. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface ISignaler Methods | Improve this Doc View Source SendMessageAsync(SignalerMessage) Send a message to the remote peer. Declaration Task SendMessageAsync(SignalerMessage message) Parameters Type Name Description SignalerMessage message The message to send. Returns Type Description Task Events | Improve this Doc View Source OnConnect Event that occurs when signaling is connected. This should fire even if the implementation is connection-less; in that case, the implementation should fire this event when it has confirmed that some message can be sent to the remote peer, even if none has been so far. Declaration event Action OnConnect Event Type Type Description Action | Improve this Doc View Source OnDisconnect Event that occurs when signaling is disconnected. This may not fire if the implementation is connection-less. Declaration event Action OnDisconnect Event Type Type Description Action | Improve this Doc View Source OnFailure Event that occurs when the signaler experiences some failure. Declaration event Action<Exception> OnFailure Event Type Type Description Action < Exception > | Improve this Doc View Source OnMessage Event that occurs when the signaler receives a new message for the local peer from a remote peer. Declaration event Action<SignalerMessage> OnMessage Event Type Type Description Action < SignalerMessage >"
  },
  "api/Microsoft.MixedReality.WebRTC.I420VideoFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420VideoFrameDelegate.html",
    "title": "Delegate I420VideoFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate I420VideoFrameDelegate Delegate used for events when an I420-encoded video frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void I420VideoFrameDelegate(I420AVideoFrame frame); Parameters Type Name Description I420AVideoFrame frame The newly available I420-encoded video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.ARGBVideoFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.ARGBVideoFrame.html",
    "title": "Struct ARGBVideoFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct ARGBVideoFrame Single video frame encoded in ARGB interleaved format (32 bits per pixel). Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct ARGBVideoFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. Fields | Improve this Doc View Source data Pointer to the data buffer containing the ARBG data for each pixel. Declaration public IntPtr data Field Value Type Description IntPtr | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source stride Stride in bytes between the ARGB rows. Declaration public int stride Field Value Type Description Int32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32"
  }
}